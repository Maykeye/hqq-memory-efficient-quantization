{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mhqq_aten package available. Set backend to HQQBackend.ATEN for faster inference and HQQBackend.ATEN_BACKPROP for faster training!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Quantizing\n",
    "from hqq.core.quantize import *\n",
    "from typing import Optional\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "# Reconstructing \n",
    "from transformers import AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(\"hqqfied\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "bin_files = list(Path(\".\").glob(\"pytorch*.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mquant_zero and quant_scale must be the same when offload_meta is set to True. Setting quant_scale=quant_zero.\u001b[0m\n",
      "\u001b[33mquant_zero and quant_scale must be the same when offload_meta is set to True. Setting quant_scale=quant_zero.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "zero_scale_group_size                              = 128\n",
    "attn_parms    = BaseQuantizeConfig(nbits=4, group_size=64, offload_meta=True) \n",
    "experts_parms = BaseQuantizeConfig(nbits=2, group_size=8, offload_meta=True)\n",
    "\n",
    "attn_parms['scale_quant_params']['group_size']    = zero_scale_group_size\n",
    "attn_parms['zero_quant_params']['group_size']     = zero_scale_group_size\n",
    "experts_parms['scale_quant_params']['group_size'] = zero_scale_group_size\n",
    "experts_parms['zero_quant_params']['group_size']  = zero_scale_group_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MixtralQuantizeConfig:\n",
    "    RE_REQUIRED_PREFIX = re.compile(r\"model\\.layers.(\\d+)\\.\")\n",
    "    RE_MOE_WEIGHT = re.compile(r\"block_sparse_moe\\.experts\\.\\d+\\.w\\d+\\.weight\")\n",
    "    RE_ATTN_WEIGHT = re.compile(r\"self_attn\\.[qkvo]_proj\\.weight\")\n",
    "    RE_NO_QUANT = re.compile(r\"\"\"\n",
    "        block_sparse_moe\\.gate.weight\n",
    "    |   input_layernorm.weight\n",
    "    |   post_attention_layernorm.weight\n",
    "    \"\"\", re.VERBOSE)\n",
    "\n",
    "    def __init__(self, attn_parms: dict, experts_parms: dict) -> None:\n",
    "        self.attn_parms = attn_parms\n",
    "        self.experts_parms = experts_parms\n",
    "\n",
    "    def quantconfig_from_tensor_name(self, full_name: str) -> Optional[dict]:\n",
    "        match = self.RE_REQUIRED_PREFIX.match(full_name)\n",
    "        if not match: return None\n",
    "        name = full_name[match.span()[1]:]\n",
    "        if self.RE_NO_QUANT.match(name): return None\n",
    "        if self.RE_MOE_WEIGHT.match(name): return self.experts_parms\n",
    "        if self.RE_ATTN_WEIGHT.match(name): return self.attn_parms\n",
    "        raise ValueError(f\"Don't know what to do with {full_name} (key={name})\")\n",
    "\n",
    "    @property\n",
    "    def compute_dtype(self):\n",
    "        return torch.bfloat16\n",
    "    \n",
    "    def tensor_name_to_layer_idx(self, tensor_name: str):\n",
    "        match = self.RE_REQUIRED_PREFIX.match(tensor_name)\n",
    "        if match is None:\n",
    "            return None\n",
    "        [start, end] = match.span(1)\n",
    "        idx = int(tensor_name[start:end])\n",
    "        return idx\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_cfg = MixtralQuantizeConfig(attn_parms=attn_parms, experts_parms=experts_parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def quantize_file(p: Path, config_getter, compute_dtype, is_secondary_tqdm=True):\n",
    "    raw_data = torch.load(p)\n",
    "    # We need to store mappings from original to quantized\n",
    "    quantized_state  = {}\n",
    "\n",
    "    for (k, v) in (bar := tqdm(raw_data.items(), leave=not is_secondary_tqdm)):\n",
    "        raw_data[k] = None # Loaded data no longer needed\n",
    "        if not (cfg := config_getter(k)):\n",
    "            quantized_state[k] = v\n",
    "            continue\n",
    "        bar.set_description(k)\n",
    "        \n",
    "        # at this point K contains name of nn.Linear.weight\n",
    "        # since Mixtral doesn't use biases we can reconstruct Linear completely\n",
    "        weight: torch.Tensor = raw_data[k]\n",
    "        out_features, in_features = weight.shape # nn.Linears weights are transposed compared to X@W, so out is shape[0]\n",
    "        linear = nn.Linear(in_features, out_features, dtype=weight.dtype, bias=False)\n",
    "        linear.weight.data = weight\n",
    "        \n",
    "        # Now we can quantize it.\n",
    "        # No need for del_orig as even if dels it, we have `linear` ourself, so HQQLinear can't delete much\n",
    "        quantized = HQQLinear(linear, cfg, compute_dtype=compute_dtype, del_orig=False) \n",
    "        quantized_state[k] = quantized.state_dict()\n",
    "        del linear\n",
    "        gc.collect() # Not sure it's needed, but probably wouldn't hurt either\n",
    "        \n",
    "\n",
    "    torch.save(quantized_state, out_dir/p.name)\n",
    "    del quantized_state\n",
    "    # Aggressive cleanup. Not sure how redundant it is, but it reduced memory usage to ~20G from ~27G when I tried 3 layers only\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in tqdm(bin_files):\n",
    "    quantize_file(p, quant_cfg.quantconfig_from_tensor_name, torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstrction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB. I restarted notebook from this cell to make sure memory is not used for anything else or selecteviely run previous\n",
    "# cells to have quant_cfg\n",
    "\n",
    "# First we need to build a dummy model. \n",
    "dummy_config = AutoConfig.from_pretrained(\".\")\n",
    "layers = dummy_config.num_hidden_layers\n",
    "# We force it to have 1 layer to not waste RAM\n",
    "dummy_config.num_hidden_layers = 1\n",
    "model = AutoModelForCausalLM.from_config(dummy_config, torch_dtype=quant_cfg.compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_files = list(out_dir.glob(\"*.bin\"))\n",
    "quantized_files.sort() # We need them to be sorted to iterate layer by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.mixtral.modeling_mixtral import MixtralDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_layer_exists(model: nn.Module, tensor_name: str):\n",
    "    tensor_name_parts = tensor_name.split('.')\n",
    "    # Find a layer or append\n",
    "    layer_name = '.'.join(tensor_name_parts[:-1])\n",
    "    layer_idx = quant_cfg.tensor_name_to_layer_idx(tensor_name)\n",
    "    if layer_idx is None:\n",
    "        return model.get_submodule(layer_name)\n",
    "    # We add one layer at the time\n",
    "    assert layer_idx <= len(model.model.layers), \"layer index must be within existing layers or 1 above\"\n",
    "    if layer_idx == len(model.model.layers):\n",
    "        new_layer = MixtralDecoderLayer(model.config, layer_idx).to(quant_cfg.compute_dtype)\n",
    "        model.model.layers.append(new_layer)\n",
    "        model.config.num_hidden_layers += 1\n",
    "    return model.get_submodule(layer_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "HQQLinear.set_backend(HQQBackend.ATEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quantized(p: Path, is_secondary_tqdm=True):\n",
    "    raw_data = torch.load(p)\n",
    "    for k, v in (bar := tqdm(raw_data.items(), leave=not is_secondary_tqdm)):\n",
    "        raw_data[k] = None\n",
    "        tensor_name_parts = k.split('.')\n",
    "        bar.set_description(k)\n",
    "        layer = ensure_layer_exists(model, k)\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            new_state_dict = {tensor_name_parts[-1] : v}\n",
    "            layer.load_state_dict(new_state_dict)\n",
    "            continue\n",
    "        assert isinstance(v, dict), \"Expected state_dict of quantized layer\"\n",
    "        linear_holder_name = '.'.join(tensor_name_parts[:-2]) # :-1 tensor holder(linear) :-2 linear holder\n",
    "        linear_holder = model.get_submodule(linear_holder_name)\n",
    "        linear_name = tensor_name_parts[-2] # [-1]=.weight [-2] = .q_proj\n",
    "        existing_linear = linear_holder.get_submodule(linear_name)\n",
    "        if isinstance(existing_linear, (nn.Linear, HQQLinear)):\n",
    "            # Need to replace\n",
    "            hqq_linear = HQQLinear(None, quant_config=quant_cfg.quantconfig_from_tensor_name(k))\n",
    "            hqq_linear.load_state_dict(v)\n",
    "            setattr(linear_holder, linear_name, hqq_linear)\n",
    "        else:\n",
    "            raise ValueError(f\"{k}: Expecting nn.Linear/HQQLinear, not {type(existing_linear)}\")\n",
    "    del raw_data\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac729e9039b9404b88a10ce4d29e9ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25172e06c0944648f02574c3aea1419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcd712d0a8a4e1c94e4e1cd4a0c513c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f41dbde730c45e191958ade4bbe24f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8015db200c3147779956b7fe8f5cc02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f840e74ef0548c687b4931e6787d0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5b031737f44d85bd0b0bc1545ba79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3363490cbabc4f58a37edcae4742d960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5192775d9140799ddbf0e15d0af758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a4fda2da7d4aa09f68b3d63c8a7470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d10733f8904d50ba3d6effa18f6a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62713a6575b74794b7e391acfdf985ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41fb7dda19e454eb628c1ae5a5fce11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26956001d64d4396b91b50a5bf3da88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536e614539fb47c18c5599d1535f7325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dd2cf0bebc4ff18eba687fbd286ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e56c59a5d74e41a93f5ba02f866724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9235eb0830c4d37b274d1b9bd596f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64af826f150b4e2785bf5fa9745170e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e560c1633e145bca579e5f750f49f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for p in tqdm(quantized_files):\n",
    "    load_quantized(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32002, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): HQQLinear()\n",
       "          (k_proj): HQQLinear()\n",
       "          (v_proj): HQQLinear()\n",
       "          (o_proj): HQQLinear()\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): HQQLinear()\n",
       "              (w2): HQQLinear()\n",
       "              (w3): HQQLinear()\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda() # 13GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer =  AutoTokenizer.from_pretrained(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Touhou. Please describe relationship between Remilia Scarlet and Sakuya Izayoi\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "y = model.generate(encoded, max_new_tokens=128)\n",
    "# With PYTORCH backend: 10m42s\n",
    "# With ATEN backend: 4m08s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|> user\n",
      "Touhou. Please describe relationship between Remilia Scarlet and Sakuya Izayoi<|im_end|> \n",
      "<|im_start|> assistant\n",
      " Remilia Scarlet is the head of the Scarlet Devil Mansion and the leader of the Scarlet Devil Team. She is a vampire and the last of the Scarlet Devil lineage. On the other hand, Sakuya Izayoi is a servant of Remilia Scarlet and the head maid of the Scarlet Devil Mansion. She is a skilled martial artist and has the ability to manipulate time and space. The relationship between Remilia Scarlet and Sakuya Izayoi is that of a master and servant, with Remilia being the master and Sakuya being the servant\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(y.ravel()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
